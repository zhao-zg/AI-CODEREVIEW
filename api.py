from dotenv import load_dotenv

load_dotenv("conf/.env")

import atexit
import json
import os
import signal
import traceback
import threading
import time
from datetime import datetime
from urllib.parse import urlparse

from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from flask import Flask, request, jsonify

from biz.gitlab.webhook_handler import slugify_url
from biz.queue.worker import handle_merge_request_event, handle_push_event, handle_github_pull_request_event, \
    handle_github_push_event
from biz.svn.svn_worker import handle_svn_changes, handle_multiple_svn_repositories
from biz.service.review_service import ReviewService
from biz.utils.im import notifier
from biz.utils.log import logger
from biz.utils.queue import handle_queue
from biz.utils.reporter import Reporter

from biz.utils.config_checker import check_config
from biz.utils.default_config import get_env_bool, get_env_with_default, get_env_int

api_app = Flask(__name__)

# å…¨å±€é…ç½®å˜é‡
push_review_enabled = get_env_bool('PUSH_REVIEW_ENABLED')
svn_check_enabled = get_env_bool('SVN_CHECK_ENABLED')

# åå°ä»»åŠ¡ç›¸å…³å…¨å±€å˜é‡
background_threads = []
scheduler = None


def reload_config():
    """é‡æ–°åŠ è½½é…ç½®"""
    global push_review_enabled, svn_check_enabled
    
    try:
        # é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv("conf/.env", override=True)
        
        # æ›´æ–°å…¨å±€é…ç½®å˜é‡
        push_review_enabled = get_env_bool('PUSH_REVIEW_ENABLED')
        svn_check_enabled = get_env_bool('SVN_CHECK_ENABLED')
        
        logger.info("APIæœåŠ¡é…ç½®å·²é‡æ–°åŠ è½½")
        print("[API] é…ç½®å·²é‡æ–°åŠ è½½")
        
    except Exception as e:
        logger.error(f"APIæœåŠ¡é‡æ–°åŠ è½½é…ç½®å¤±è´¥: {e}")
        print(f"[API] é‡æ–°åŠ è½½é…ç½®å¤±è´¥: {e}")


def setup_signal_handlers():
    """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
    def signal_handler(signum, frame):
        if hasattr(signal, 'SIGUSR1') and signum == signal.SIGUSR1:
            print("[API] æ”¶åˆ°é…ç½®é‡è½½ä¿¡å· (SIGUSR1)")
            reload_config()
        elif signum == signal.SIGTERM:
            print("[API] æ”¶åˆ°ç»ˆæ­¢ä¿¡å· (SIGTERM)")
            # è¿™é‡Œå¯ä»¥æ·»åŠ ä¼˜é›…å…³é—­é€»è¾‘
            shutdown_background_tasks()
        elif hasattr(signal, 'SIGHUP') and signum == signal.SIGHUP:
            print("[API] æ”¶åˆ°é‡å¯ä¿¡å· (SIGHUP)")
            reload_config()
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼ˆä»…åœ¨æ”¯æŒçš„ç³»ç»Ÿä¸Šï¼‰
    if hasattr(signal, 'SIGUSR1'):
        signal.signal(signal.SIGUSR1, signal_handler)  # é…ç½®é‡è½½
    signal.signal(signal.SIGTERM, signal_handler)  # ä¼˜é›…å…³é—­
    if hasattr(signal, 'SIGHUP'):
        signal.signal(signal.SIGHUP, signal_handler)   # é‡å¯/é‡è½½


# è®¾ç½®ä¿¡å·å¤„ç†å™¨
setup_signal_handlers()

@api_app.route('/')
def home():
    svn_status = "å·²å¯ç”¨" if svn_check_enabled else "æœªå¯ç”¨"
    
    # è·å–SVNä»“åº“ä¿¡æ¯
    svn_info = ""
    if svn_check_enabled:
        import json
        svn_repositories_config = get_env_with_default('SVN_REPOSITORIES')
        try:
            repositories = json.loads(svn_repositories_config)
            if repositories:
                svn_info = f"<p><strong>é…ç½®çš„SVNä»“åº“:</strong></p><ul>"
                for repo in repositories:
                    name = repo.get('name', 'unknown')
                    url = repo.get('remote_url', 'unknown')
                    svn_info += f"<li>{name}: {url}</li>"
                svn_info += "</ul>"
            else:
                # æ£€æŸ¥å•ä»“åº“é…ç½®
                svn_remote_url = get_env_with_default('SVN_REMOTE_URL')
                if svn_remote_url:
                    svn_info = f"<p><strong>SVNä»“åº“:</strong> {svn_remote_url}</p>"
        except json.JSONDecodeError:
            svn_info = "<p><strong>SVNé…ç½®:</strong> é…ç½®è§£æé”™è¯¯</p>"
    
    return f"""<h2>AIä»£ç å®¡æŸ¥æœåŠ¡æ­£åœ¨è¿è¡Œ</h2>
              <p><strong>SVNå®šæ—¶æ£€æŸ¥åŠŸèƒ½ï¼š</strong> {svn_status}</p>
              {svn_info}              <p><strong>GitHubé¡¹ç›®åœ°å€:</strong> <a href="https://github.com/zhao-zg/AI-CODEREVIEW" target="_blank">
              https://github.com/zhao-zg/AI-CODEREVIEW</a></p>
              <p><strong>Dockeré•œåƒ:</strong> <a href="https://github.com/zhao-zg/AI-CODEREVIEW/pkgs/container/ai-codereview" target="_blank">
              ghcr.io/zhao-zg/ai-codereview</a></p>
              <p><strong>æ”¯æŒçš„åŠŸèƒ½:</strong></p>
              <ul>
                <li>GitLab Webhook è§¦å‘å®¡æŸ¥</li>
                <li>GitHub Webhook è§¦å‘å®¡æŸ¥</li>
                <li>SVN å®šæ—¶æ£€æŸ¥å®¡æŸ¥ï¼ˆæ”¯æŒå¤šä»“åº“ï¼‰</li>
                <li>æ‰‹åŠ¨è§¦å‘ SVN æ£€æŸ¥: <a href="/svn/check" target="_blank">POST /svn/check</a></li>
                <li>æ‰‹åŠ¨è§¦å‘æŒ‡å®šä»“åº“: <a href="/svn/check?repo=ä»“åº“å" target="_blank">POST /svn/check?repo=ä»“åº“å</a></li>
              </ul>
              """


@api_app.route('/health')
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({
        "status": "healthy",
        "message": "AI Code Review service is running",
        "timestamp": datetime.now().isoformat()
    })


@api_app.route('/review/daily_report', methods=['GET'])
def daily_report():
    # è·å–å½“å‰æ—¥æœŸ0ç‚¹å’Œ23ç‚¹59åˆ†59ç§’çš„æ—¶é—´æˆ³
    start_time = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0).timestamp()
    end_time = datetime.now().replace(hour=23, minute=59, second=59, microsecond=0).timestamp()

    try:
        if push_review_enabled:
            df = ReviewService().get_push_review_logs(updated_at_gte=start_time, updated_at_lte=end_time)
        else:
            df = ReviewService().get_mr_review_logs(updated_at_gte=start_time, updated_at_lte=end_time)

        if df.empty:
            logger.info("No data to process.")
            return jsonify({'message': 'No data to process.'}), 200
        # å»é‡ï¼šåŸºäº (author, message) ç»„åˆ
        df_unique = df.drop_duplicates(subset=["author", "commit_messages"])
        # æŒ‰ç…§ author æ’åº
        df_sorted = df_unique.sort_values(by="author")
        # è½¬æ¢ä¸ºé€‚åˆç”Ÿæˆæ—¥æŠ¥çš„æ ¼å¼
        commits = df_sorted.to_dict(orient="records")
        # ç”Ÿæˆæ—¥æŠ¥å†…å®¹
        report_txt = Reporter().generate_report(json.dumps(commits))
        # å‘é€é’‰é’‰é€šçŸ¥
        notifier.send_notification(content=report_txt, msg_type="markdown", title="ä»£ç æäº¤æ—¥æŠ¥")

        # è¿”å›ç”Ÿæˆçš„æ—¥æŠ¥å†…å®¹
        return json.dumps(report_txt, ensure_ascii=False, indent=4)
    except Exception as e:
        logger.error(f"Failed to generate daily report: {e}")
        return jsonify({'message': f"Failed to generate daily report: {e}"}), 500


def setup_scheduler():
    """
    é…ç½®å¹¶å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
    """
    global scheduler
    
    try:
        scheduler = BackgroundScheduler()
          # æ—¥æŠ¥å®šæ—¶ä»»åŠ¡
        crontab_expression = get_env_with_default('REPORT_CRONTAB_EXPRESSION')
        logger.info(f"ğŸ“… Reading cron expression: '{crontab_expression}'")
        cron_parts = crontab_expression.split()
        logger.info(f"ğŸ“‹ Cron parts after split: {cron_parts} (count: {len(cron_parts)})")
        
        # éªŒè¯cronè¡¨è¾¾å¼æ ¼å¼
        if len(cron_parts) != 5:
            logger.error(f"âŒ Invalid cron expression format: '{crontab_expression}'. Expected 5 parts (minute hour day month day_of_week), got {len(cron_parts)}")
            logger.info(f"ğŸ’¡ Using default cron expression: '0 18 * * 1-5'")
            cron_parts = '0 18 * * 1-5'.split()
        
        cron_minute, cron_hour, cron_day, cron_month, cron_day_of_week = cron_parts
        logger.info(f"âœ… Cron schedule set: minute={cron_minute}, hour={cron_hour}, day={cron_day}, month={cron_month}, day_of_week={cron_day_of_week}")

        scheduler.add_job(
            daily_report,
            trigger=CronTrigger(
                minute=cron_minute,
                hour=cron_hour,
                day=cron_day,
                month=cron_month,
                day_of_week=cron_day_of_week
            )        )
        
        # SVNå®šæ—¶æ£€æŸ¥ä»»åŠ¡
        if svn_check_enabled:
            svn_crontab = get_env_with_default('SVN_CHECK_CRONTAB')  # é»˜è®¤æ¯30åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            svn_cron_parts = svn_crontab.split()
            
            if len(svn_cron_parts) == 5:
                svn_minute, svn_hour, svn_day, svn_month, svn_day_of_week = svn_cron_parts
                
                scheduler.add_job(
                    trigger_svn_check,
                    trigger=CronTrigger(
                        minute=svn_minute,
                        hour=svn_hour,
                        day=svn_day,
                        month=svn_month,
                        day_of_week=svn_day_of_week
                    )
                )
                logger.info(f"SVNå®šæ—¶æ£€æŸ¥ä»»åŠ¡å·²é…ç½®ï¼Œå®šæ—¶è¡¨è¾¾å¼: {svn_crontab}")
            else:
                logger.error(f"SVNå®šæ—¶è¡¨è¾¾å¼æ ¼å¼é”™è¯¯: {svn_crontab}")

        # Start the scheduler
        scheduler.start()
        logger.info("Scheduler started successfully.")        # Shut down the scheduler when exiting the app
        atexit.register(lambda: scheduler.shutdown())
        
    except Exception as e:
        logger.error(f"âŒ Error setting up scheduler: {e}")
        logger.error(f"âŒ Traceback: {traceback.format_exc()}")


# å¤„ç† GitLab Merge Request Webhook
@api_app.route('/review/webhook', methods=['POST'])
def handle_webhook():
    # è·å–è¯·æ±‚çš„JSONæ•°æ®
    if request.is_json:
        data = request.get_json()
        if not data:
            return jsonify({"error": "Invalid JSON"}), 400

        # åˆ¤æ–­æ˜¯GitLabè¿˜æ˜¯GitHubçš„webhook
        webhook_source = request.headers.get('X-GitHub-Event')

        if webhook_source:  # GitHub webhook
            return handle_github_webhook(webhook_source, data)
        else:  # GitLab webhook
            return handle_gitlab_webhook(data)
    else:
        return jsonify({'message': 'Invalid data format'}), 400


def handle_github_webhook(event_type, data):    # è·å–GitHubé…ç½®
    github_token = get_env_with_default('GITHUB_ACCESS_TOKEN') or request.headers.get('X-GitHub-Token')
    if not github_token:
        return jsonify({'message': 'Missing GitHub access token'}), 400

    github_url = get_env_with_default('GITHUB_URL') or 'https://github.com'
    github_url_slug = slugify_url(github_url)

    # æ‰“å°æ•´ä¸ªpayloadæ•°æ®
    logger.info(f'Received GitHub event: {event_type}')
    logger.info(f'Payload: {json.dumps(data)}')

    if event_type == "pull_request":
        # ä½¿ç”¨handle_queueè¿›è¡Œå¼‚æ­¥å¤„ç†
        handle_queue(handle_github_pull_request_event, data, github_token, github_url, github_url_slug)
        # ç«‹é©¬è¿”å›å“åº”
        return jsonify(
            {'message': f'GitHub request received(event_type={event_type}), will process asynchronously.'}), 200
    elif event_type == "push":
        # ä½¿ç”¨handle_queueè¿›è¡Œå¼‚æ­¥å¤„ç†
        handle_queue(handle_github_push_event, data, github_token, github_url, github_url_slug)
        # ç«‹é©¬è¿”å›å“åº”
        return jsonify(
            {'message': f'GitHub request received(event_type={event_type}), will process asynchronously.'}), 200
    else:
        error_message = f'Only pull_request and push events are supported for GitHub webhook, but received: {event_type}.'
        logger.error(error_message)
        return jsonify(error_message), 400


def handle_gitlab_webhook(data):
    object_kind = data.get("object_kind")    # ä¼˜å…ˆä»è¯·æ±‚å¤´è·å–ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡è·å–ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»æ¨é€äº‹ä»¶ä¸­è·å–
    gitlab_url = get_env_with_default('GITLAB_URL') or request.headers.get('X-Gitlab-Instance')
    if not gitlab_url:
        repository = data.get('repository')
        if not repository:
            return jsonify({'message': 'Missing GitLab URL'}), 400
        homepage = repository.get("homepage")
        if not homepage:
            return jsonify({'message': 'Missing GitLab URL'}), 400
        try:
            parsed_url = urlparse(homepage)
            gitlab_url = f"{parsed_url.scheme}://{parsed_url.netloc}/"
        except Exception as e:
            return jsonify({"error": f"Failed to parse homepage URL: {str(e)}"}), 400    # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å–ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»è¯·æ±‚å¤´è·å–
    gitlab_token = get_env_with_default('GITLAB_ACCESS_TOKEN') or request.headers.get('X-Gitlab-Token')
    # å¦‚æœgitlab_tokenä¸ºç©ºï¼Œè¿”å›é”™è¯¯
    if not gitlab_token:
        return jsonify({'message': 'Missing GitLab access token'}), 400

    gitlab_url_slug = slugify_url(gitlab_url)

    # æ‰“å°æ•´ä¸ªpayloadæ•°æ®ï¼Œæˆ–æ ¹æ®éœ€æ±‚è¿›è¡Œå¤„ç†
    logger.info(f'Received event: {object_kind}')
    logger.info(f'Payload: {json.dumps(data)}')

    # å¤„ç†Merge Request Hook
    if object_kind == "merge_request":
        # åˆ›å»ºä¸€ä¸ªæ–°è¿›ç¨‹è¿›è¡Œå¼‚æ­¥å¤„ç†
        handle_queue(handle_merge_request_event, data, gitlab_token, gitlab_url, gitlab_url_slug)
        # ç«‹é©¬è¿”å›å“åº”
        return jsonify(
            {'message': f'Request received(object_kind={object_kind}), will process asynchronously.'}), 200
    elif object_kind == "push":
        # åˆ›å»ºä¸€ä¸ªæ–°è¿›ç¨‹è¿›è¡Œå¼‚æ­¥å¤„ç†
        # TODO check if PUSH_REVIEW_ENABLED is needed here
        handle_queue(handle_push_event, data, gitlab_token, gitlab_url, gitlab_url_slug)
        # ç«‹é©¬è¿”å›å“åº”
        return jsonify(
            {'message': f'Request received(object_kind={object_kind}), will process asynchronously.'}), 200
    else:
        error_message = f'Only merge_request and push events are supported (both Webhook and System Hook), but received: {object_kind}.'
        logger.error(error_message)
        return jsonify(error_message), 400


@api_app.route('/svn/check', methods=['GET', 'POST'])
def manual_svn_check():
    """æ‰‹åŠ¨è§¦å‘SVNæ£€æŸ¥"""
    if not svn_check_enabled:
        return jsonify({'message': 'SVNæ£€æŸ¥åŠŸèƒ½æœªå¯ç”¨'}), 400
    
    try:
        # å…è®¸é€šè¿‡æŸ¥è¯¢å‚æ•°è¦†ç›–æ£€æŸ¥çš„å°æ—¶æ•°
        hours_str = request.args.get('hours')
        if hours_str:
            try:
                hours = int(hours_str)
            except ValueError:
                return jsonify({'message': 'å‚æ•° "hours" å¿…é¡»æ˜¯æ•´æ•°'}), 400
        else:
            hours = None

        # å…è®¸é€šè¿‡æŸ¥è¯¢å‚æ•°æŒ‡å®šä»“åº“åç§°
        repo_name = request.args.get('repo')
        
        # å¼‚æ­¥å¤„ç†SVNæ£€æŸ¥
        if repo_name:
            # æ£€æŸ¥ç‰¹å®šä»“åº“
            handle_queue(trigger_specific_svn_repo, repo_name=repo_name, hours=hours)
            message = f'ä»“åº“ "{repo_name}" çš„SVNæ£€æŸ¥å·²å¯åŠ¨'
        else:
            # æ£€æŸ¥æ‰€æœ‰ä»“åº“
            handle_queue(trigger_svn_check, hours=hours)
            message = 'SVNæ£€æŸ¥å·²å¯åŠ¨'
        
        # å‡†å¤‡å“åº”æ¶ˆæ¯
        if hours is not None:
            message += f'ï¼Œå°†å¼‚æ­¥å¤„ç†æœ€è¿‘ {hours} å°æ—¶çš„æäº¤ã€‚'
        else:
            default_hours = get_env_with_default('SVN_CHECK_INTERVAL_HOURS')
            message += f'ï¼Œå°†å¼‚æ­¥å¤„ç†æœ€è¿‘ {default_hours} å°æ—¶çš„æäº¤ã€‚'

        return jsonify({'message': message}), 200
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨è§¦å‘SVNæ£€æŸ¥å¤±è´¥: {e}")
        return jsonify({'message': f'æ‰‹åŠ¨è§¦å‘SVNæ£€æŸ¥å¤±è´¥: {e}'}), 500


def trigger_specific_svn_repo(repo_name: str, hours: int = None):
    """è§¦å‘ç‰¹å®šSVNä»“åº“çš„æ£€æŸ¥"""
    if not svn_check_enabled:
        logger.info("SVNæ£€æŸ¥åŠŸèƒ½æœªå¯ç”¨")
        return
    
    import json
    
    # è·å–ä»“åº“é…ç½®
    svn_repositories_config = get_env_with_default('SVN_REPOSITORIES')
    try:
        repositories = json.loads(svn_repositories_config)
    except json.JSONDecodeError as e:
        logger.error(f"SVNä»“åº“é…ç½®JSONè§£æå¤±è´¥: {e}")
        return
    
    # æŸ¥æ‰¾æŒ‡å®šçš„ä»“åº“
    target_repo = None
    for repo_config in repositories:
        if repo_config.get('name') == repo_name:
            target_repo = repo_config
            break
    
    if not target_repo:
        logger.error(f"æœªæ‰¾åˆ°åä¸º '{repo_name}' çš„ä»“åº“é…ç½®")
        return
    
    # è·å–ä»“åº“é…ç½®
    remote_url = target_repo.get('remote_url')
    local_path = target_repo.get('local_path')
    username = target_repo.get('username')
    password = target_repo.get('password')
    repo_check_hours = hours or target_repo.get('check_hours', 24)
    
    if not remote_url or not local_path:
        logger.error(f"ä»“åº“ {repo_name} é…ç½®ä¸å®Œæ•´")
        return
    
    logger.info(f"å¼€å§‹æ£€æŸ¥æŒ‡å®šä»“åº“: {repo_name}")
    handle_svn_changes(remote_url, local_path, username, password, repo_check_hours, repo_name)


def trigger_svn_check(hours: int = None):
    """è§¦å‘SVNæ£€æŸ¥"""
    if not svn_check_enabled:
        logger.info("SVNæ£€æŸ¥åŠŸèƒ½æœªå¯ç”¨")
        return
      # ä¼˜å…ˆä½¿ç”¨å¤šä»“åº“é…ç½®
    # è·å–å…¨å±€è®¾ç½®
    check_limit = get_env_int('SVN_CHECK_LIMIT')    
    svn_repositories_config = get_env_with_default('SVN_REPOSITORIES')
    if svn_repositories_config and svn_repositories_config.strip() != '[]':
        logger.info("ä½¿ç”¨å¤šä»“åº“é…ç½®è¿›è¡ŒSVNæ£€æŸ¥")
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        logger.debug(f"SVN_REPOSITORIES é…ç½®é•¿åº¦: {len(svn_repositories_config)}")
        logger.debug(f"å‰50å­—ç¬¦: {repr(svn_repositories_config[:50])}")
        logger.debug(f"å10å­—ç¬¦: {repr(svn_repositories_config[-10:])}")
        
        # æ£€æµ‹å¯èƒ½çš„å•å¼•å·é—®é¢˜
        if "'" in svn_repositories_config and '"' not in svn_repositories_config:
            logger.warning("âš ï¸ æ£€æµ‹åˆ°é…ç½®ä¸­ä½¿ç”¨äº†å•å¼•å·ï¼ŒJSONè¦æ±‚ä½¿ç”¨åŒå¼•å·")
        
        handle_multiple_svn_repositories(svn_repositories_config, hours, check_limit)
        return
      # å›é€€åˆ°å•ä»“åº“é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
    svn_remote_url = get_env_with_default('SVN_REMOTE_URL')
    svn_local_path = get_env_with_default('SVN_LOCAL_PATH')
    
    if not svn_remote_url or not svn_local_path:
        logger.error("SVN_REPOSITORIES æˆ– SVN_REMOTE_URL+SVN_LOCAL_PATH ç¯å¢ƒå˜é‡å¿…é¡»è®¾ç½®")
        return
    
    svn_username = get_env_with_default('SVN_USERNAME')
    svn_password = get_env_with_default('SVN_PASSWORD')
      # å¦‚æœæœªæä¾›å°æ—¶æ•°ï¼Œä»ç¯å¢ƒå˜é‡è¯»å–é»˜è®¤å€¼
    if hours is None:
        check_hours = get_env_int('SVN_CHECK_INTERVAL_HOURS')
    else:
        check_hours = hours
    
    logger.info(f"ä½¿ç”¨å•ä»“åº“é…ç½®è¿›è¡ŒSVNæ£€æŸ¥ï¼Œè¿œç¨‹URL: {svn_remote_url}, æœ¬åœ°è·¯å¾„: {svn_local_path}, æ£€æŸ¥æœ€è¿‘ {check_hours} å°æ—¶")
    handle_svn_changes(svn_remote_url, svn_local_path, svn_username, svn_password, check_hours, check_limit)


# æ·»åŠ é…ç½®é‡è½½çš„APIç«¯ç‚¹
@api_app.route('/reload-config', methods=['POST'])
def reload_config_endpoint():
    """é…ç½®é‡è½½APIç«¯ç‚¹"""
    try:
        reload_config()
        return jsonify({
            "success": True,
            "message": "é…ç½®å·²æˆåŠŸé‡æ–°åŠ è½½",
            "timestamp": datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({
            "success": False,
            "message": f"é…ç½®é‡è½½å¤±è´¥: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }), 500


def start_background_tasks():
    """å¯åŠ¨åå°ä»»åŠ¡ï¼ˆå•æœåŠ¡æ¶æ„ï¼‰"""
    global background_threads
    
    logger.info("ğŸš€ åˆå§‹åŒ–åå°ä»»åŠ¡...")
    
    # å¯åŠ¨ SVN åå°ä»»åŠ¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    svn_enabled = get_env_bool('SVN_CHECK_ENABLED')
    if svn_enabled:
        try:
            from biz.svn.svn_worker import main as svn_main
            
            def svn_worker_thread():
                """SVNå·¥ä½œçº¿ç¨‹"""
                try:
                    logger.info("ğŸš€ å¯åŠ¨ SVN åå°ä»»åŠ¡å¤„ç†å™¨")
                    # åªæ‰§è¡Œä¸€æ¬¡ï¼Œç”±è°ƒåº¦å™¨æ§åˆ¶é¢‘ç‡
                    svn_main()
                except Exception as e:
                    logger.error(f"âŒ SVN ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            
            # åœ¨å•ç‹¬çº¿ç¨‹ä¸­å¯åŠ¨SVNä»»åŠ¡
            thread = threading.Thread(target=svn_worker_thread, daemon=True)
            thread.start()
            background_threads.append(thread)
            logger.info("âœ… SVN åå°ä»»åŠ¡å·²å¯åŠ¨")
            
        except ImportError as e:
            logger.error(f"âŒ SVN åå°ä»»åŠ¡å¯åŠ¨å¤±è´¥ (ç¼ºå°‘ä¾èµ–): {e}")
        except Exception as e:
            logger.error(f"âŒ SVN åå°ä»»åŠ¡å¯åŠ¨å¤±è´¥: {e}")
    else:
        logger.info("â„¹ï¸ SVN æ£€æŸ¥å·²ç¦ç”¨")
    
    logger.info("âœ… åå°ä»»åŠ¡åˆå§‹åŒ–å®Œæˆ")

def shutdown_background_tasks():
    """å…³é—­åå°ä»»åŠ¡"""
    global background_threads, scheduler
    
    logger.info("â¹ï¸ æ­£åœ¨å…³é—­åå°ä»»åŠ¡...")
    
    # å…³é—­è°ƒåº¦å™¨
    if scheduler:
        scheduler.shutdown()
    
    # ç­‰å¾…åå°çº¿ç¨‹ç»“æŸ
    for thread in background_threads:
        if thread.is_alive():
            logger.info(f"ç­‰å¾…çº¿ç¨‹ {thread.name} ç»“æŸ...")
            thread.join(timeout=5)
    
    logger.info("âœ… åå°ä»»åŠ¡å·²å…³é—­")


if __name__ == '__main__':
    try:
        logger.info("ğŸš€ å¯åŠ¨ AI-CodeReview ç»Ÿä¸€æœåŠ¡")
        
        check_config()
        
        # å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
        setup_scheduler()
        
        # å¯åŠ¨åå°ä»»åŠ¡
        start_background_tasks()
        
        # å¯åŠ¨Flask APIæœåŠ¡
        port = get_env_int('SERVER_PORT')
        logger.info(f"ğŸŒ å¯åŠ¨ Flask API æœåŠ¡ï¼Œç«¯å£: {port}")
        
        # æ³¨å†Œä¼˜é›…å…³é—­å¤„ç†
        atexit.register(shutdown_background_tasks)
        
        api_app.run(host='0.0.0.0', port=port)
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡...")
        shutdown_background_tasks()
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        shutdown_background_tasks()
        raise
